{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351fe5db-0cc0-4971-888b-8857ec9e2e61",
   "metadata": {},
   "source": [
    "# Titanic survivor prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca771b40-07fa-408f-8e09-07456ceaf7ad",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7a6825-b1d1-4780-ac3a-0df59d88894c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import joblib\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    BaggingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5442a9a0-b04a-4269-b4e9-f535588f7933",
   "metadata": {},
   "source": [
    "!conda update --all -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a3ff9-b3e9-46df-946e-26c26f80d0c1",
   "metadata": {},
   "source": [
    "Link to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78c0f081-cc75-4824-8659-366580eb9bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0fff4-4d6f-4ddc-97ca-016e9b7dce10",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d117400b-ec79-40eb-8da2-e9764e28d479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (891, 12)\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "\n",
      "Missing Values in Each Column:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Dataset Shape:\", data.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "data.info()\n",
    "\n",
    "\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1496a7b7-7abe-4212-85d4-d61f459b03da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "\n",
      "Unique Values per Column:\n",
      "PassengerId: 891 unique values\n",
      "Survived: 2 unique values\n",
      "Pclass: 3 unique values\n",
      "Name: 891 unique values\n",
      "Sex: 2 unique values\n",
      "Age: 88 unique values\n",
      "SibSp: 7 unique values\n",
      "Parch: 7 unique values\n",
      "Ticket: 681 unique values\n",
      "Fare: 248 unique values\n",
      "Cabin: 147 unique values\n",
      "Embarked: 3 unique values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "\n",
    "print(\"\\nUnique Values per Column:\")\n",
    "for column in data.columns:\n",
    "    print(f\"{column}: {data[column].nunique()} unique values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c9543-5c2f-4372-ab87-6c14d3a4bb57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(data.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b77cf-77a8-4b53-9958-d6168e669628",
   "metadata": {},
   "source": [
    "I think I will drop the cabin, and I'm not sure I want to plug in data to the \"age\" since that is one of the key variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55bba8b-637c-41a7-9291-2a2d831a55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of rows before dropping\n",
    "print(f\"Initial dataset shape: {data.shape}\")\n",
    "\n",
    "# Drop the 'Cabin' column\n",
    "data_cleaned = data.drop(columns=['Cabin'])\n",
    "print(\"Dropped 'Cabin' column.\")\n",
    "\n",
    "# Display the number of missing 'Age' values\n",
    "missing_age = data_cleaned['Age'].isnull().sum()\n",
    "print(f\"Number of missing 'Age' values before dropping: {missing_age}\")\n",
    "\n",
    "# Drop rows with missing 'Age' values\n",
    "data_cleaned = data_cleaned.dropna(subset=['Age'])\n",
    "print(\"Dropped rows with missing 'Age' values.\")\n",
    "\n",
    "# Display the dataset shape after dropping\n",
    "print(f\"Dataset shape after dropping: {data_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc271a0-699b-41aa-a77b-eb4660dfef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_threshold = 21\n",
    "\n",
    "# Create the 'Child' column: True if Age < 16, else False\n",
    "data_cleaned['Child'] = data_cleaned['Age'] < age_threshold\n",
    "print(\"Added 'Child' column based on 'Age'.\")\n",
    "\n",
    "# Display the distribution of the 'Child' feature\n",
    "child_counts = data_cleaned['Child'].value_counts()\n",
    "print(\"\\nDistribution of 'Child' feature:\")\n",
    "print(child_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2339bc30-f8c1-41d5-aee4-d6342a05e9d7",
   "metadata": {},
   "source": [
    "I will also drop the unnecessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a11c66-30b6-4cfb-9c7e-2fed1b1b7c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['PassengerId', 'Name', 'Ticket']  # Ticket is like a code, we don't need it, FARE is the one that is interesting\n",
    "\n",
    "X = data_cleaned.drop(columns=columns_to_drop + ['Survived'])  # 'Survived' is the target\n",
    "y = data_cleaned['Survived']\n",
    "\n",
    "print(\"Updated Features shape:\", X.shape)\n",
    "print(\"Updated Target shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f49d7-1c20-4511-8ff6-a7c0ba6f22f4",
   "metadata": {},
   "source": [
    "noice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4076a3c1-2aab-425a-91c5-2991c68bace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405970f7-3781-4a01-b6ca-4344be6c91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29b410-91ac-4176-8b57-a1f9fa137246",
   "metadata": {},
   "source": [
    "## Lets check the outliers, and do some plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce529e5-3369-4336-a75b-124bb64470f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create histograms and density plots\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data[feature], kde=True, bins=30, color='skyblue')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    \n",
    "    # Density Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.kdeplot(data[feature], shade=True, color='navy')\n",
    "    plt.title(f'Density Plot of {feature}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a6b34-8386-43fb-8c00-f00244f31254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create boxplots for numerical features\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=data[feature], color='lightgreen')\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422cdd4-2ec5-4c40-b803-46704a77058c",
   "metadata": {},
   "source": [
    "there is one guy with 500 so we will drop that one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cfd04f-5622-4edc-8b6a-3f8cedd66e59",
   "metadata": {},
   "source": [
    "## Now we encode the one that need encoding to numeral, and also normalize the numerical ones: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a65ad6-f8c5-43d3-9763-7d833cdc6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_features = ['Sex', 'Embarked', 'Pclass']\n",
    "\n",
    "# Convert 'Pclass' to string to treat it as a categorical variable\n",
    "X['Pclass'] = X['Pclass'].astype(str)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "print(\"Performed one-hot encoding on categorical features.\")\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Identify numerical features after encoding\n",
    "numerical_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "# Scale numerical features\n",
    "X_encoded[numerical_features] = scaler.fit_transform(X_encoded[numerical_features])\n",
    "print(\"Scaled numerical features.\")\n",
    "\n",
    "# Display the first few rows of the processed data\n",
    "X_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd3344f-a356-4819-aa29-68df65262e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all boolean columns\n",
    "boolean_columns = X_encoded.select_dtypes(include='bool').columns\n",
    "\n",
    "# Convert boolean columns to numeric (0/1)\n",
    "X_encoded[boolean_columns] = X_encoded[boolean_columns].astype(int)\n",
    "\n",
    "X = X_encoded\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77030140-7c27-41d5-9fe8-f95c72871ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping outliers\n",
    "outlier_indices = data_cleaned[data_cleaned['Fare'] >= 500].index\n",
    "data_cleaned = data_cleaned.drop(index=outlier_indices)\n",
    "X_encoded = X_encoded.drop(index=outlier_indices)\n",
    "y = y.drop(index=outlier_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7aefc9-ea08-4f0b-a61f-42693f0af629",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26dc2f-0fd6-4c6d-b54f-8d37f69f4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outlier_fare = data_cleaned[data_cleaned['Fare'] >= 500]\n",
    "print(\"Outliers with Fare >= 500:\")\n",
    "print(outlier_fare)\n",
    "\n",
    "outlier_indices = outlier_fare.index\n",
    "print(f\"Indices of outliers to be dropped: {outlier_indices.tolist()}\")\n",
    "\n",
    "# Drop the outlier(s) from data_cleaned\n",
    "data_cleaned = data_cleaned.drop(index=outlier_indices)\n",
    "\n",
    "# Update X_encoded and y_final accordingly\n",
    "X = X_encoded.drop(index=outlier_indices)\n",
    "y = y.drop(index=outlier_indices)\n",
    "\n",
    "print(f\"Dataset shape after dropping outliers: {X_encoded.shape}\")\n",
    "print(f\"Target shape after dropping outliers: {y.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3d21f-75d1-4616-bced-494c6c7c10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = X_encoded.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=cmap, square=True, linewidths=.5)\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcb1d6-2a5d-4959-8166-36ead2c5f127",
   "metadata": {},
   "source": [
    "## SPLITTING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4694ce29-0d6f-4646-a320-f8316f9dca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=52, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {df_X_train.shape}\")\n",
    "print(f\"Test set size: {df_X_test.shape}\")\n",
    "print(f\"Target set size: {df_y_train.shape}\")\n",
    "print(f\"Target set size: {df_y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e8db5-0e53-4d8a-941f-0246042d230a",
   "metadata": {},
   "source": [
    "### Here is what I want to figure out: What was the age where they were considered a child? In other words, when is there a bump up in survivability rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999fc27-466e-4638-af1a-8a80bad5d47e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_child_age_cutoff_v2(df, age_column='Age', target_column='Survived', age_min=1, age_max=100):\n",
    "    \"\"\"\n",
    "    Determines the age cutoff where the largest increase in survivability occurs by training separate\n",
    "    logistic regression models for each age threshold.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        age_column (str): The name of the age column.\n",
    "        target_column (str): The name of the target column indicating survival.\n",
    "        age_min (int): The minimum age to consider.\n",
    "        age_max (int): The maximum age to consider.\n",
    "    \n",
    "    Returns:\n",
    "        cutoff_age (int): The age at which the largest increase in survivability occurs.\n",
    "        results_df (pd.DataFrame): DataFrame containing age cutoffs and corresponding survival probabilities.\n",
    "    \"\"\"\n",
    "    survival_probs = []\n",
    "    ages = np.arange(age_min, age_max + 1)\n",
    "    \n",
    "    for age in ages:\n",
    "        # Create binary feature: is_child\n",
    "        df['is_child'] = df[age_column] <= age\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = df[['is_child']]\n",
    "        y = df[target_column]\n",
    "        \n",
    "        # Initialize and fit logistic regression model\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Predict survival probability for is_child=True\n",
    "        prob_child_survival = model.predict_proba([[1]])[0][1]\n",
    "        survival_probs.append(prob_child_survival)\n",
    "    \n",
    "    # Create a DataFrame for results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Age_Cutoff': ages,\n",
    "        'Survival_Probability_Children': survival_probs\n",
    "    })\n",
    "    \n",
    "    # Calculate differences between consecutive ages\n",
    "    results_df['Survival_Probability_Diff'] = results_df['Survival_Probability_Children'].diff()\n",
    "    \n",
    "    # Identify the age with the maximum increase in survivability\n",
    "    cutoff_age = results_df['Survival_Probability_Diff'].idxmax()  # Index of max diff\n",
    "    cutoff_age_value = results_df.loc[cutoff_age, 'Age_Cutoff']\n",
    "    \n",
    "    # Plot Survival Probability vs Age Cutoff\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=results_df, x='Age_Cutoff', y='Survival_Probability_Children', marker='o')\n",
    "    plt.title('Survival Probability of Children by Age Cutoff')\n",
    "    plt.xlabel('Age Cutoff')\n",
    "    plt.ylabel('Predicted Survival Probability')\n",
    "    plt.grid(True)\n",
    "    plt.axvline(x=cutoff_age_value, color='red', linestyle='--', label=f'Cutoff Age = {cutoff_age_value}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Difference in Survival Probability\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=results_df, x='Age_Cutoff', y='Survival_Probability_Diff', palette='viridis')\n",
    "    plt.title('Change in Survival Probability Between Consecutive Age Cutoffs')\n",
    "    plt.xlabel('Age Cutoff')\n",
    "    plt.ylabel('Change in Survival Probability')\n",
    "    plt.grid(True)\n",
    "    plt.axvline(x=cutoff_age_value, color='red', linestyle='--', label=f'Cutoff Age = {cutoff_age_value}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"The age cutoff with the largest increase in survivability is at age {cutoff_age_value}.\")\n",
    "    \n",
    "    # Drop the temporary 'is_child' column\n",
    "    df.drop(columns=['is_child'], inplace=True)\n",
    "    \n",
    "    return cutoff_age_value, results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce74453-e84b-4aa7-870d-0cb28ac7eecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cutoff_age, results_df = find_child_age_cutoff_v2(\n",
    "    data, \n",
    "    age_column='Age', \n",
    "    target_column='Survived', \n",
    "    age_min=1, \n",
    "    age_max=40\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc95c23-150d-48aa-a5ab-6041ea8da00a",
   "metadata": {},
   "source": [
    "### Looky looky! IT really starts going up at 21! I find that relevant, thus, I will now go back and change the child age to 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe312c4-402e-41e0-b49b-469a4e9c39da",
   "metadata": {},
   "source": [
    "## Lets find the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eec665-05ea-4f49-bce5-ed3a9cdaef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model_classification(df_X_train, df_X_test, df_y_train, df_y_test):\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    # Try importing additional models; if not installed, they will be skipped.\n",
    "    try:\n",
    "        from xgboost import XGBClassifier\n",
    "    except ImportError:\n",
    "        XGBClassifier = None\n",
    "    try:\n",
    "        from lightgbm import LGBMClassifier\n",
    "    except ImportError:\n",
    "        LGBMClassifier = None\n",
    "    try:\n",
    "        from catboost import CatBoostClassifier\n",
    "    except ImportError:\n",
    "        CatBoostClassifier = None\n",
    "\n",
    "    # Define a list of diverse classification models and configurations:\n",
    "    models = [\n",
    "        (\"Logistic Regression (L2, C=1)\", LogisticRegression(penalty='l2', C=1, solver='lbfgs', \n",
    "                                                              max_iter=1000, random_state=42)),\n",
    "        (\"Logistic Regression (L2, C=0.1)\", LogisticRegression(penalty='l2', C=0.1, solver='lbfgs', \n",
    "                                                                max_iter=1000, random_state=42)),\n",
    "        (\"Logistic Regression (L2, C=10)\", LogisticRegression(penalty='l2', C=10, solver='lbfgs', \n",
    "                                                               max_iter=1000, random_state=42)),\n",
    "        (\"Logistic Regression (L1, C=1)\", LogisticRegression(penalty='l1', C=1, solver='liblinear', \n",
    "                                                              max_iter=1000, random_state=42)),\n",
    "        (\"Decision Tree Classifier\", DecisionTreeClassifier(random_state=42)),\n",
    "        (\"Random Forest Classifier\", RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        (\"Extra Trees Classifier\", ExtraTreesClassifier(random_state=42)),\n",
    "        (\"Gradient Boosting Classifier\", GradientBoostingClassifier(random_state=42)),\n",
    "        (\"AdaBoost Classifier\", AdaBoostClassifier(random_state=42)),\n",
    "        (\"Bagging Classifier\", BaggingClassifier(random_state=42)),\n",
    "        (\"SVC\", SVC(probability=True, random_state=42)),\n",
    "        (\"Linear SVC\", LinearSVC(max_iter=1000, random_state=42)),\n",
    "        (\"K-Nearest Neighbors Classifier\", KNeighborsClassifier()),\n",
    "        (\"Gaussian NB\", GaussianNB()),\n",
    "        (\"Bernoulli NB\", BernoulliNB()),\n",
    "        (\"Ridge Classifier\", RidgeClassifier()),\n",
    "        (\"Passive Aggressive Classifier\", PassiveAggressiveClassifier(max_iter=1000, random_state=42)),\n",
    "        (\"Quadratic Discriminant Analysis\", QuadraticDiscriminantAnalysis()),\n",
    "        (\"MLP Classifier\", MLPClassifier(max_iter=1000, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    if XGBClassifier is not None:\n",
    "        models.append((\"XGBoost Classifier\", \n",
    "                       XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)))\n",
    "    if LGBMClassifier is not None:\n",
    "        models.append((\"LightGBM Classifier\", LGBMClassifier(random_state=42)))\n",
    "    if CatBoostClassifier is not None:\n",
    "        models.append((\"CatBoost Classifier\", CatBoostClassifier(verbose=0, random_state=42)))\n",
    "\n",
    "    best_model = None\n",
    "    best_auc = -np.inf\n",
    "    best_model_name = None\n",
    "    best_model_instance = None\n",
    "\n",
    "    for name, model in models:\n",
    "        # Train the model\n",
    "        model.fit(df_X_train, df_y_train)\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(df_X_test)\n",
    "        try:\n",
    "            # Some models might not have predict_proba; if so, skip ROC AUC\n",
    "            y_pred_proba = model.predict_proba(df_X_test)[:, 1]\n",
    "            roc_auc = roc_auc_score(df_y_test, y_pred_proba)\n",
    "        except AttributeError:\n",
    "            roc_auc = None\n",
    "\n",
    "        acc = accuracy_score(df_y_test, y_pred)\n",
    "        f1 = f1_score(df_y_test, y_pred)\n",
    "        \n",
    "        print(f\"{name} Model:\")\n",
    "        print(\"Accuracy:\", acc)\n",
    "        if roc_auc is not None:\n",
    "            print(\"ROC AUC Score:\", roc_auc)\n",
    "        else:\n",
    "            print(\"ROC AUC Score: Not available\")\n",
    "        print(\"F1 Score:\", f1)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(df_y_test, y_pred))\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(df_y_test, y_pred))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Update best model based on ROC AUC if available; otherwise, use F1-score as backup\n",
    "        if roc_auc is not None:\n",
    "            if roc_auc > best_auc:\n",
    "                best_auc = roc_auc\n",
    "                best_model = name\n",
    "                best_model_instance = model\n",
    "        else:\n",
    "            if f1 > best_auc:\n",
    "                best_auc = f1\n",
    "                best_model = name\n",
    "                best_model_instance = model\n",
    "\n",
    "    print(\"The best model is:\", best_model, \"with a ROC AUC Score of\", best_auc)\n",
    "    # Print the metrics for the best model\n",
    "    y_pred_best = best_model_instance.predict(df_X_test)\n",
    "    best_acc = accuracy_score(df_y_test, y_pred_best)\n",
    "    best_f1 = f1_score(df_y_test, y_pred_best)\n",
    "    print(\"Metrics of the best model:\")\n",
    "    print(\"Accuracy:\", best_acc)\n",
    "    print(\"F1 Score:\", best_f1)\n",
    "    print(\"ROC AUC Score:\", best_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67dd37e-7428-4a0e-b1ee-de619732518a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_best_model_classification(df_X_train, df_X_test, df_y_train, df_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e35cf9-8411-4b37-9fe9-44a45f27854e",
   "metadata": {},
   "source": [
    "###  First Result\n",
    "\n",
    "The best model is: Gradient Boosting Classifier with a ROC AUC Score of 0.8672413793103447\n",
    "\n",
    "Metrics of the best model:\n",
    "\n",
    "Accuracy: 0.7972027972027972\n",
    "\n",
    "F1 Score: 0.7128712871287128\n",
    "\n",
    "ROC AUC Score: 0.8672413793103447\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eee157-3663-4c4b-bacb-416e77264379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_feature_selection(df, target_column, test_size=0.3, auc_threshold=0.01, max_features=None):\n",
    "    \"\"\"\n",
    "    Perform forward feature selection for a classification task.\n",
    "    \n",
    "    This function iteratively evaluates which explanatory variable improves the ROC AUC\n",
    "    the most when added to the current feature set, using a Logistic Regression classifier.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The full DataFrame containing all features and the target.\n",
    "        target_column (str): The name of the target column.\n",
    "        test_size (float): Fraction of the data to use for testing in each iteration.\n",
    "        auc_threshold (float): Minimum improvement in ROC AUC required to continue adding features.\n",
    "        max_features (int or None): Maximum number of features to select. If None, all features are considered.\n",
    "        \n",
    "    Returns:\n",
    "        selected_features (list): List of selected feature names.\n",
    "        metrics_history (dict): History of the selection process with iteration index as key.\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    # Separate out target and features\n",
    "    y = df[target_column]\n",
    "    X = df.drop(columns=[target_column])\n",
    "    \n",
    "    # Initialize lists and metrics storage\n",
    "    remaining_features = list(X.columns)\n",
    "    selected_features = []\n",
    "    best_auc_global = 0  # Starting from 0\n",
    "    metrics_history = {}\n",
    "\n",
    "    # Set maximum number of features to select if not provided\n",
    "    if max_features is None:\n",
    "        max_features = len(remaining_features)\n",
    "\n",
    "    for i in range(max_features):\n",
    "        best_auc_this_round = 0\n",
    "        best_feature = None\n",
    "        \n",
    "        # Test each remaining feature to see which gives the best improvement\n",
    "        for feature in remaining_features:\n",
    "            candidate_features = selected_features + [feature]\n",
    "            X_subset = X[candidate_features]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_subset, y, test_size=test_size, random_state=42)\n",
    "            model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            current_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            if current_auc > best_auc_this_round:\n",
    "                best_auc_this_round = current_auc\n",
    "                best_feature = feature\n",
    "        \n",
    "        # Check if the best improvement in this round is significant enough; if not, stop selecting further features.\n",
    "        if best_auc_this_round - best_auc_global < auc_threshold:\n",
    "            print(f\"Stopping: Improvement in ROC AUC below threshold of {auc_threshold}.\")\n",
    "            break\n",
    "        \n",
    "        # Update selected features and metrics\n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "        best_auc_global = best_auc_this_round\n",
    "        metrics_history[len(selected_features)] = {\"Feature\": best_feature, \"ROC_AUC\": best_auc_global}\n",
    "        print(f\"Iteration {len(selected_features)}: Selected '{best_feature}' with ROC AUC = {best_auc_global:.4f}\")\n",
    "    \n",
    "    return selected_features, metrics_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b69d8-8f43-4388-80f9-6e2aeeda080a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def combine_X_y(X, y, target_name=\"Survived\"):\n",
    "    \"\"\"\n",
    "    Combines feature matrix X and target vector y into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (pd.Series or pd.DataFrame): Target vector.\n",
    "        target_name (str): Name of the target column in the combined DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame containing both features and target.\n",
    "    \"\"\"\n",
    "    # Ensure that y is a DataFrame (convert if necessary)\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_frame(name=target_name)\n",
    "    elif isinstance(y, pd.DataFrame) and target_name not in y.columns:\n",
    "        y = y.rename(columns={y.columns[0]: target_name})\n",
    "\n",
    "    # Reset indices to ensure alignment\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "\n",
    "    # Concatenate X and y\n",
    "    combined_df = pd.concat([X, y], axis=1)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Example Usage\n",
    "df = combine_X_y(X, y)\n",
    "print(\"Combined DataFrame shape:\", df.shape)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae4d7e-141a-45da-ab50-8d0cc93e4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_features, metrics_history = forward_feature_selection(df, target_column='Survived', \n",
    "                                                                test_size=0.3, auc_threshold=0.01, max_features=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6ce45-6ac2-495f-a465-96ceb228e2b9",
   "metadata": {},
   "source": [
    "welp, okay, let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662d09c-e8cd-423d-a2ba-33b4936cf401",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Sex_male', 'Pclass_3', 'Survived']\n",
    "df_1 = df[selected_columns]\n",
    "\n",
    "# Split the new DataFrame into training and testing sets\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(\n",
    "    df_1.drop(columns=['Survived']), \n",
    "    df_1['Survived'], \n",
    "    test_size=0.3, \n",
    "    random_state=52, \n",
    "    stratify=df_1['Survived']\n",
    ")\n",
    "\n",
    "# Display the shapes of the splits\n",
    "print(f\"Training set shape: {df_X_train.shape}, {df_y_train.shape}\")\n",
    "print(f\"Testing set shape: {df_X_test.shape}, {df_y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ad74f-c908-4056-84b4-0f18d5745bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_model_classification(df_X_train, df_X_test, df_y_train, df_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828611f-7d4a-4693-bf19-2d7d62ad3787",
   "metadata": {},
   "source": [
    "### Second Result:\n",
    "\n",
    "\n",
    "The best model is: MLP Classifier with a ROC AUC Score of 0.8507449127906976\n",
    "\n",
    "Metrics of the best model:\n",
    "\n",
    "Accuracy: 0.794392523364486\n",
    "\n",
    "F1 Score: 0.6716417910447762\n",
    "\n",
    "ROC AUC Score: 0.8507449127906976\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca5a8e9-56ea-4f06-933d-9fbdf9023f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features, metrics_history = forward_feature_selection(df, target_column='Survived', \n",
    "                                                                test_size=0.3, auc_threshold=0.001, max_features=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b60d7-b80f-4b46-8ab4-2f726d0f4c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['Sex_male', 'Pclass_3', 'SibSp', 'Fare', 'Child', 'Pclass_2']\n",
    "\n",
    "# Create a new DataFrame with the selected features and the target variable\n",
    "df_2 = df[selected_features + ['Survived']]\n",
    "\n",
    "# Split the new DataFrame into training and testing sets\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(\n",
    "    df_2.drop(columns=['Survived']), \n",
    "    df_2['Survived'], \n",
    "    test_size=0.3, \n",
    "    random_state=52, \n",
    "    stratify=df_2['Survived']\n",
    ")\n",
    "\n",
    "# Display the shapes of the splits\n",
    "print(f\"Training set shape: {df_X_train.shape}, {df_y_train.shape}\")\n",
    "print(f\"Testing set shape: {df_X_test.shape}, {df_y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276f333-9b5d-4f94-a4c8-d1899af7458f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_best_model_classification(df_X_train, df_X_test, df_y_train, df_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920d89a-7f14-4b0b-b507-849df25093db",
   "metadata": {},
   "source": [
    "### Third result:\n",
    "\n",
    "The best model is: MLP Classifier with a ROC AUC Score of 0.8675508720930233\n",
    "\n",
    "Metrics of the best model:\n",
    "\n",
    "Accuracy: 0.8177570093457944\n",
    "\n",
    "F1 Score: 0.7483870967741936\n",
    "\n",
    "ROC AUC Score: 0.8675508720930233"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf25281-f850-4e48-9f95-8b251e4ebed0",
   "metadata": {},
   "source": [
    "Looks like we go with number 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991fce8-4d0d-4b2a-b88f-1864c3d509c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "    df: pd.DataFrame,\n",
    "    selected_features: list,\n",
    "    target: str = 'Survived',\n",
    "    model=None,\n",
    "    test_size: float = 0.3,\n",
    "    random_state: int = 52,\n",
    "    cv: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Trains a classification model, makes predictions, evaluates performance metrics,\n",
    "    and performs cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing features and target.\n",
    "    - selected_features (list): List of feature column names to be used.\n",
    "    - target (str): The name of the target variable column. Default is 'Survived'.\n",
    "    - model: The machine learning model to be trained. If None, RidgeClassifier is used.\n",
    "    - test_size (float): Proportion of the dataset to include in the test split. Default is 0.3.\n",
    "    - random_state (int): Random state for reproducibility. Default is 52.\n",
    "    - cv (int): Number of cross-validation folds. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    - metrics (dict): A dictionary containing all evaluated metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use RidgeClassifier if no model is provided\n",
    "    if model is None:\n",
    "        model = RidgeClassifier()\n",
    "        print(\"No model provided. Using RidgeClassifier by default.\")\n",
    "    else:\n",
    "        print(f\"Using model: {model.__class__.__name__}\")\n",
    "\n",
    "    # 1. Select Features and Target\n",
    "    try:\n",
    "        df_selected = df[selected_features + [target]]\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"One of the selected features or target not found in DataFrame: {e}\")\n",
    "\n",
    "    # 2. Split the Data into Training and Testing Sets\n",
    "    X = df_selected.drop(columns=[target])\n",
    "    y = df_selected[target]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, \n",
    "        y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set shape: {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "    # 3. Train the Model\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"\\nModel training completed.\")\n",
    "\n",
    "    # 4. Make Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # 5. Evaluate Metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Handle ROC AUC depending on model capabilities\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        y_test_proba = model.decision_function(X_test)\n",
    "    else:\n",
    "        # If neither method is available, use predictions as probabilities\n",
    "        y_test_proba = y_test_pred\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "    # 6. Cross-Validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    mean_cv_accuracy = cv_scores.mean()\n",
    "\n",
    "    # 7. Print Metrics\n",
    "    print(\"\\n=== Model Evaluation Metrics ===\")\n",
    "    print(f\"Accuracy on Training Data: {train_accuracy:.4f}\")\n",
    "    print(f\"Accuracy on Testing Data: {test_accuracy:.4f}\")\n",
    "    print(f\"F1 Score on Training Data: {train_f1:.4f}\")\n",
    "    print(f\"F1 Score on Testing Data: {test_f1:.4f}\")\n",
    "    print(f\"ROC AUC Score on Testing Data: {roc_auc:.4f}\")\n",
    "    print(f\"Cross-Validation Accuracy Scores: {cv_scores}\")\n",
    "    print(f\"Mean Cross-Validation Accuracy: {mean_cv_accuracy:.4f}\")\n",
    "\n",
    "    # 8. Return Metrics\n",
    "    metrics = {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_f1': train_f1,\n",
    "        'test_f1': test_f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'cv_scores': cv_scores,\n",
    "        'mean_cv_accuracy': mean_cv_accuracy\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f16794-5d32-4031-b17f-fb57c9da59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Assuming you have a DataFrame `df` already loaded\n",
    "    # Define selected features\n",
    "    selected_features = ['Sex_male', 'Pclass_3', 'SibSp', 'Fare', 'Child', 'Pclass_2']\n",
    "    \n",
    "    # Initialize the best model (MLP Classifier)\n",
    "    best_model = MLPClassifier(random_state=52)\n",
    "    \n",
    "    # Call the testing function\n",
    "    metrics = test_model(\n",
    "        df=df,\n",
    "        selected_features=selected_features,\n",
    "        target='Survived',\n",
    "        model=best_model,  # Here you can switch to other classifiers\n",
    "        test_size=0.3,\n",
    "        random_state=52,\n",
    "        cv=5\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9e28d-4bfe-4aed-89bd-1e59d602111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Over_Fitting_Inquiry(*dfs, target_column, test_size=0.2, random_state=1, names=None, overfitting_threshold=0.1):\n",
    "    \"\"\"\n",
    "    This function evaluates overfitting and performance-related metrics for multiple DataFrames using the Ridge Classifier.\n",
    "    It computes key metrics for each dataset, generates a detailed summary, and provides a recommendation for the best model,\n",
    "    considering both performance and generalization capabilities.\n",
    "\n",
    "    Key Features:\n",
    "    - **Model Evaluation**:\n",
    "      Computes metrics including Training/Test Accuracy, F1 Score, Precision, Recall, ROC AUC (if applicable), and Cross-Validation Accuracy.\n",
    "    - **Overfitting Analysis**:\n",
    "      Detects overfitting based on the gap between training and test performance (Performance Gap). Models with significant overfitting are penalized.\n",
    "    - **Aggregated Score**:\n",
    "      Combines Test Accuracy, Test F1 Score, Mean CV Accuracy, and Test ROC AUC (if available) into a single score. Penalizes overfitted models.\n",
    "    - **Analytical Summary**:\n",
    "      Generates a table summarizing all metrics for easy comparison across datasets.\n",
    "    - **Best Model Recommendation**:\n",
    "      Identifies the best-performing model based on the Aggregated Score while ensuring it is not overfitted.\n",
    "\n",
    "    Parameters:\n",
    "        *dfs: One or more pandas DataFrames, each containing explanatory variables and the target column.\n",
    "        target_column (str): The name of the target column (binary classification is required).\n",
    "        test_size (float): Fraction of the data to allocate for the test split (default is 0.2).\n",
    "        random_state (int): Random seed for reproducibility (default is 1).\n",
    "        names (list or None): Optional list of custom names for the DataFrames. If None, default names (e.g., df_1, df_2, ...) are assigned.\n",
    "        overfitting_threshold (float): Threshold for the acceptable gap between training and test performance \n",
    "                                       before penalizing the model's Aggregated Score (default is 0.1).\n",
    "\n",
    "    Returns:\n",
    "        results (dict): A dictionary where each key corresponds to a DataFrame name, and each value is a dictionary of metrics:\n",
    "            - **Training Accuracy**: Accuracy on the training set.\n",
    "            - **Test Accuracy**: Accuracy on the test set.\n",
    "            - **Training F1 Score**: F1 Score on the training set.\n",
    "            - **Test F1 Score**: F1 Score on the test set.\n",
    "            - **Training Precision**: Precision on the training set.\n",
    "            - **Test Precision**: Precision on the test set.\n",
    "            - **Training Recall**: Recall on the training set.\n",
    "            - **Test Recall**: Recall on the test set.\n",
    "            - **Training ROC AUC**: ROC AUC on the training set (if applicable).\n",
    "            - **Test ROC AUC**: ROC AUC on the test set (if applicable).\n",
    "            - **Cross-Validation Scores**: Array of accuracy scores from 5-fold cross-validation on the training set.\n",
    "            - **Mean CV Accuracy**: Mean accuracy from cross-validation scores.\n",
    "            - **Performance Gap**: Difference between Training and Test Accuracy.\n",
    "            - **Aggregated Score**: Overall score combining key metrics, penalized for overfitting.\n",
    "            - **Analysis**: A brief interpretation of the model's performance, highlighting generalization or overfitting issues.\n",
    "\n",
    "    Output:\n",
    "        - **Detailed Metrics for Each Dataset**:\n",
    "          Displays all metrics and their values for each DataFrame.\n",
    "        - **Analytical Summary Table**:\n",
    "          Provides a tabular comparison of key metrics (Test Accuracy, Test F1 Score, Mean CV Accuracy, Test ROC AUC, Aggregated Score, and Analysis).\n",
    "        - **Best Model Suggestion**:\n",
    "          Recommends the model with the highest Aggregated Score while ensuring it generalizes well and is not overfitted.\n",
    "\n",
    "    Notes:\n",
    "    - Models flagged as overfitting based on the `overfitting_threshold` will be penalized and deprioritized.\n",
    "    - If all models exhibit overfitting, the function will recommend revisiting the model complexity or dataset quality.\n",
    "    - Default scoring for cross-validation is Accuracy; this can be adjusted to other metrics if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import RidgeClassifier\n",
    "    from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score,\n",
    "                                 precision_score, recall_score)\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Handle naming: auto-generate names if none or too few are provided.\n",
    "    if names is None:\n",
    "        names = [f\"df_{i+1}\" for i in range(len(dfs))]\n",
    "    else:\n",
    "        names = list(names)\n",
    "        if len(names) < len(dfs):\n",
    "            for i in range(len(dfs) - len(names)):\n",
    "                names.append(f\"df_{len(names) + i + 1}\")\n",
    "        elif len(names) > len(dfs):\n",
    "            print(\"Warning: More names provided than DataFrames. Extra names will be ignored.\")\n",
    "            names = names[:len(dfs)]\n",
    "    \n",
    "    # Process each DataFrame\n",
    "    for idx, df in enumerate(dfs):\n",
    "        current_name = names[idx]\n",
    "        \n",
    "        # Ensure target exists\n",
    "        if target_column not in df.columns:\n",
    "            print(f\"Error: Target column '{target_column}' not found in '{current_name}'. Skipping this DataFrame.\")\n",
    "            continue\n",
    "        \n",
    "        # Extract features and target\n",
    "        X = df.drop(columns=[target_column])\n",
    "        y = df[target_column]\n",
    "        \n",
    "        # Check if target is binary\n",
    "        if y.nunique() != 2:\n",
    "            print(f\"Error: Target column '{target_column}' in '{current_name}' is not binary. Skipping this DataFrame.\")\n",
    "            continue\n",
    "        \n",
    "        # Split the data\n",
    "        df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # Initialize and train the model\n",
    "        model = RidgeClassifier()\n",
    "        model.fit(df_X_train, df_y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = model.predict(df_X_train)\n",
    "        y_test_pred = model.predict(df_X_test)\n",
    "        \n",
    "        # Basic metrics\n",
    "        train_accuracy = accuracy_score(df_y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(df_y_test, y_test_pred)\n",
    "        train_f1 = f1_score(df_y_train, y_train_pred)\n",
    "        test_f1 = f1_score(df_y_test, y_test_pred)\n",
    "        train_precision = precision_score(df_y_train, y_train_pred, zero_division=0)\n",
    "        test_precision = precision_score(df_y_test, y_test_pred, zero_division=0)\n",
    "        train_recall = recall_score(df_y_train, y_train_pred, zero_division=0)\n",
    "        test_recall = recall_score(df_y_test, y_test_pred, zero_division=0)\n",
    "        \n",
    "        # ROC AUC using decision_function if possible\n",
    "        try:\n",
    "            y_train_proba = model.decision_function(df_X_train)\n",
    "            y_test_proba = model.decision_function(df_X_test)\n",
    "            train_roc_auc = roc_auc_score(df_y_train, y_train_proba)\n",
    "            test_roc_auc = roc_auc_score(df_y_test, y_test_proba)\n",
    "        except Exception as e:\n",
    "            train_roc_auc = None\n",
    "            test_roc_auc = None\n",
    "        \n",
    "        # Cross-validation on training set\n",
    "        cv_scores = cross_val_score(model, df_X_train, df_y_train, cv=5, scoring='accuracy')\n",
    "        cv_mean = np.mean(cv_scores)\n",
    "        \n",
    "        # Compute Performance Gap\n",
    "        performance_gap = train_accuracy - test_accuracy\n",
    "        \n",
    "        # -------------------------------\n",
    "        # Compute Aggregated Score with Penalties\n",
    "        # -------------------------------\n",
    "        # Base score is an average of key metrics\n",
    "        metrics_to_average = [test_accuracy, test_f1, cv_mean]\n",
    "        if test_roc_auc is not None:\n",
    "            metrics_to_average.append(test_roc_auc)\n",
    "        \n",
    "        # Calculate base score\n",
    "        base_score = sum(metrics_to_average) / len(metrics_to_average)\n",
    "        \n",
    "        # Apply penalty for overfitting\n",
    "        penalty = 0\n",
    "        if performance_gap > overfitting_threshold:\n",
    "            penalty = performance_gap * 0.5  # Adjust the multiplier as needed\n",
    "            analysis_gap = performance_gap\n",
    "        else:\n",
    "            analysis_gap = 0\n",
    "        \n",
    "        # Final Aggregated Score\n",
    "        aggregated_score = base_score - penalty\n",
    "        \n",
    "        # Overfitting Analysis\n",
    "        if performance_gap > overfitting_threshold:\n",
    "            analysis = (f\"Overfitting detected (Performance Gap: {round(performance_gap, 4)}). \"\n",
    "                        \"Aggregated score penalized.\")\n",
    "        elif (test_accuracy - train_accuracy) > overfitting_threshold:\n",
    "            analysis = (f\"Possible underfitting (Performance Gap: {round(performance_gap, 4)}). \"\n",
    "                        \"Consider model complexity or data quality.\")\n",
    "        else:\n",
    "            analysis = \"Good generalization.\"\n",
    "        \n",
    "        # Store everything in results\n",
    "        results[current_name] = {\n",
    "            \"Training Accuracy\": round(train_accuracy, 4),\n",
    "            \"Test Accuracy\": round(test_accuracy, 4),\n",
    "            \"Training F1 Score\": round(train_f1, 4),\n",
    "            \"Test F1 Score\": round(test_f1, 4),\n",
    "            \"Training Precision\": round(train_precision, 4),\n",
    "            \"Test Precision\": round(test_precision, 4),\n",
    "            \"Training Recall\": round(train_recall, 4),\n",
    "            \"Test Recall\": round(test_recall, 4),\n",
    "            \"Training ROC AUC\": round(train_roc_auc, 4) if train_roc_auc is not None else \"Not Available\",\n",
    "            \"Test ROC AUC\": round(test_roc_auc, 4) if test_roc_auc is not None else \"Not Available\",\n",
    "            \"Cross-Validation Scores\": [round(score, 4) for score in cv_scores],\n",
    "            \"Mean CV Accuracy\": round(cv_mean, 4),\n",
    "            \"Performance Gap\": round(performance_gap, 4),\n",
    "            \"Aggregated Score\": round(aggregated_score, 4),\n",
    "            \"Analysis\": analysis\n",
    "        }\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Print out individual results\n",
    "    # -------------------------------\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"Results for {name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Build an analytical summary table\n",
    "    # -------------------------------\n",
    "    print(\"=== Analytical Summary ===\")\n",
    "    summary = []\n",
    "    for name, metrics in results.items():\n",
    "        summary.append({\n",
    "            \"Name\": name,\n",
    "            \"Test Accuracy\": metrics[\"Test Accuracy\"],\n",
    "            \"Test F1 Score\": metrics[\"Test F1 Score\"],\n",
    "            \"Mean CV Accuracy\": metrics[\"Mean CV Accuracy\"],\n",
    "            \"Test ROC AUC\": metrics[\"Test ROC AUC\"],\n",
    "            \"Aggregated Score\": metrics[\"Aggregated Score\"],\n",
    "            \"Analysis\": metrics[\"Analysis\"]\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame for better formatting\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    \n",
    "    # Display the summary table\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Choose a winner based on Aggregated Score\n",
    "    # -------------------------------\n",
    "    if len(summary) > 0:\n",
    "        # Exclude models that are overfitting\n",
    "        non_overfitting_models = [model for model in summary if \"Overfitting detected\" not in model[\"Analysis\"]]\n",
    "        \n",
    "        if non_overfitting_models:\n",
    "            # Select the model with the highest Aggregated Score among non-overfitting models\n",
    "            best_model = max(non_overfitting_models, key=lambda x: x[\"Aggregated Score\"])\n",
    "            print(\"\\n=== Best Model Suggestion ===\")\n",
    "            print(\n",
    "                f\"The best model is '{best_model['Name']}' \"\n",
    "                f\"with an Aggregated Score of {best_model['Aggregated Score']}. \\n\"\n",
    "                \"This indicates strong overall performance across Test Accuracy, F1 Score, Mean CV Accuracy, \"\n",
    "                \"and (if available) ROC AUC, without signs of overfitting.\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\n=== Best Model Suggestion ===\")\n",
    "            print(\"All models exhibit signs of overfitting based on the defined threshold. Consider revisiting model complexity or data quality.\")\n",
    "    else:\n",
    "        print(\"\\nNo valid DataFrames were processed. No winner can be selected.\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80668f86-04ec-40d1-a8c0-ae9e94f9b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Over_Fitting_Inquiry(df, df_1, df_2, target_column='Survived', test_size=0.2, random_state=52, names=['Model1', 'Model2', 'Model3'], overfitting_threshold=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b324904-b984-4b18-bccf-f33318db8549",
   "metadata": {},
   "source": [
    "Okily dokily thats df_2 still"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8c246-d0a2-4137-a635-12b6597c2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target_column, test_size=0.2, random_state=52):\n",
    "    \"\"\"\n",
    "    Splits the input DataFrame into training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to split.\n",
    "        target_column (str): The name of the target column.\n",
    "        test_size (float): The fraction of the dataset to include in the test split (default is 0.2).\n",
    "        random_state (int): Random seed for reproducibility (default is 42).\n",
    "    \n",
    "    Returns:\n",
    "        X_train (pd.DataFrame): Training set features.\n",
    "        X_test (pd.DataFrame): Test set features.\n",
    "        y_train (pd.Series): Training set target.\n",
    "        y_test (pd.Series): Test set target.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return df_X_train, df_X_test, df_y_train, df_y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2cad8c-a768-4168-9b46-3466eaa45eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data(df, 'Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be24d44-859c-483b-beb7-edb39c14b16c",
   "metadata": {},
   "source": [
    "## Let's do the parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744c432-2ca7-42d3-b034-3fc05315ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def tune_mlp_classifier(\n",
    "    df: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 52,\n",
    "    param_grid: dict = None,\n",
    "    scoring: str = 'accuracy',\n",
    "    cv: int = 5,\n",
    "    n_jobs: int = -1,\n",
    "    overfitting_threshold: float = 0.1,\n",
    "    complexity: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tunes an MLPClassifier using GridSearchCV on the provided DataFrame and evaluates it with multiple metrics.\n",
    "    \n",
    "    The evaluation uses:\n",
    "      - Test Accuracy\n",
    "      - Test F1 Score\n",
    "      - Mean CV Accuracy (via stratified cross-validation)\n",
    "      - Test ROC AUC (if available; computed via predict_proba)\n",
    "      \n",
    "    An aggregated score is computed as the average of these metrics. If the training-test\n",
    "    performance gap exceeds 'overfitting_threshold', a penalty is applied.\n",
    "\n",
    "    The 'complexity' parameter (1 to 10) controls how extensive the parameter grid is:\n",
    "      - Lower = simpler architectures and fewer hyperparameter variations\n",
    "      - Higher = more complex architectures and broader hyperparameter variations\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing features and the target column.\n",
    "        target_column (str): Name of the target column in df (binary classification).\n",
    "        test_size (float): Fraction for test split (default 0.2).\n",
    "        random_state (int): Seed for reproducibility (default 52).\n",
    "        param_grid (dict or None): Custom grid. If None, dynamically generated based on 'complexity'.\n",
    "        scoring (str): Scoring metric for GridSearchCV (default 'accuracy').\n",
    "        cv (int): Number of folds for cross-validation (default 5).\n",
    "        n_jobs (int): Number of jobs to run in parallel (-1 uses all processors).\n",
    "        overfitting_threshold (float): Gap threshold for penalizing overfitting (default 0.1).\n",
    "        complexity (int): 1 to 10, controlling the range/granularity of hyperparameters in the grid (default 5).\n",
    "        \n",
    "    Returns:\n",
    "        best_estimator (Pipeline): The best Pipeline (Scaler + MLPClassifier) model found.\n",
    "        final_aggregated_score (float): The aggregated score (after overfitting penalty).\n",
    "        best_params (dict): The best hyperparameters as a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Dynamically build a parameter grid if none is provided\n",
    "    if param_grid is None:\n",
    "        # Define hidden_layer_sizes based on complexity\n",
    "        if complexity <= 3:\n",
    "            hidden_layer_sizes = [(50,), (100,)]\n",
    "            alpha_values = [0.0001, 0.001, 0.01]\n",
    "            activation_options = ['relu', 'tanh']\n",
    "            solver_options = ['adam']\n",
    "            learning_rate_inits = [0.001, 0.01]\n",
    "        elif complexity <= 7:\n",
    "            hidden_layer_sizes = [(100,), (100, 50), (150, 100, 50)]\n",
    "            alpha_values = np.logspace(-4, -1, num=6).tolist()  # 0.0001 to 0.1\n",
    "            activation_options = ['relu', 'tanh', 'logistic']\n",
    "            solver_options = ['adam', 'sgd']\n",
    "            learning_rate_inits = [0.001, 0.01, 0.1]\n",
    "        else:  # complexity 8-10\n",
    "            hidden_layer_sizes = [(100,), (150,), (100, 100), (150, 100, 50)]\n",
    "            alpha_values = np.logspace(-4, 1, num=10).tolist()  # 0.0001 to 10\n",
    "            activation_options = ['relu', 'tanh', 'logistic', 'identity']\n",
    "            solver_options = ['adam', 'sgd', 'lbfgs']\n",
    "            learning_rate_inits = [0.0001, 0.001, 0.01, 0.1]\n",
    "        \n",
    "        param_grid = {\n",
    "            'mlp__hidden_layer_sizes': hidden_layer_sizes,\n",
    "            'mlp__alpha': alpha_values,\n",
    "            'mlp__activation': activation_options,\n",
    "            'mlp__solver': solver_options,\n",
    "            'mlp__learning_rate_init': learning_rate_inits\n",
    "        }\n",
    "    \n",
    "    # 2. Prepare data + stratified split\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Check if target is binary\n",
    "    if y.nunique() != 2:\n",
    "        raise ValueError(f\"Target column '{target_column}' is not binary. It has {y.nunique()} unique values.\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    \n",
    "    print(f\"\\nTraining set shape: {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")\n",
    "    \n",
    "    # 3. Define Pipeline with StandardScaler and MLPClassifier\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('mlp', MLPClassifier(random_state=random_state, max_iter=200))\n",
    "    ])\n",
    "    \n",
    "    # 4. Define StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # 5. Grid Search on MLPClassifier within the pipeline\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=skf,\n",
    "        scoring=scoring,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=1,\n",
    "        refit=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStarting Grid Search...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"Grid Search Completed.\")\n",
    "    \n",
    "    # Retrieve best estimator and details\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    best_cv_score = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # 6. Evaluate best estimator on test set\n",
    "    y_test_pred = best_estimator.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "    \n",
    "    # Overfitting analysis\n",
    "    y_train_pred = best_estimator.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    performance_gap = train_accuracy - test_accuracy\n",
    "    \n",
    "    # 7. Compute Test ROC AUC if possible\n",
    "    try:\n",
    "        y_test_proba = best_estimator.predict_proba(X_test)[:, 1]\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    except Exception as e:\n",
    "        test_roc_auc = None\n",
    "        print(f\"ROC AUC could not be computed: {e}\")\n",
    "    \n",
    "    # Cross-validation with best_estimator\n",
    "    cv_scores = cross_val_score(best_estimator, X_train, y_train, cv=skf, scoring=scoring, n_jobs=n_jobs)\n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "    \n",
    "    # 8. Compute aggregated score\n",
    "    metrics_list = [test_accuracy, test_f1, mean_cv_accuracy]\n",
    "    if test_roc_auc is not None:\n",
    "        metrics_list.append(test_roc_auc)\n",
    "    base_score = sum(metrics_list) / len(metrics_list)\n",
    "    \n",
    "    # Overfitting penalty\n",
    "    penalty = 0\n",
    "    if performance_gap > overfitting_threshold:\n",
    "        penalty = 0.5 * performance_gap\n",
    "    final_aggregated_score = base_score - penalty\n",
    "    \n",
    "    # 9. Create analysis message\n",
    "    if performance_gap > overfitting_threshold:\n",
    "        analysis_message = (f\"Overfitting detected (Performance Gap: {performance_gap:.4f}). \"\n",
    "                            f\"Penalty applied: {penalty:.4f}.\")\n",
    "    elif (test_accuracy - train_accuracy) > overfitting_threshold:\n",
    "        analysis_message = (f\"Possible underfitting (Performance Gap: {performance_gap:.4f}). \"\n",
    "                            \"Test performance is higher than training performance.\")\n",
    "    else:\n",
    "        analysis_message = \"Good generalization.\"\n",
    "    \n",
    "    # 10. Printing the results\n",
    "    print(\"\\n=== Grid Search Results ===\")\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(f\"Best Cross-Validation Score ({scoring}): {best_cv_score:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Mean CV Accuracy: {mean_cv_accuracy:.4f}\")\n",
    "    if test_roc_auc is not None:\n",
    "        print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "    else:\n",
    "        print(\"Test ROC AUC: Not Available\")\n",
    "    print(f\"Performance Gap (Train - Test Accuracy): {performance_gap:.4f}\")\n",
    "    print(f\"Aggregated Score (before penalty): {base_score:.4f}\")\n",
    "    print(f\"Overfitting Penalty: {penalty:.4f}\")\n",
    "    print(f\"Final Aggregated Score: {final_aggregated_score:.4f}\")\n",
    "    print(f\"Analysis: {analysis_message}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nDetailed Classification Report on Test Set:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    \n",
    "    # Grid search CV results\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    if 'display' in globals():\n",
    "        from IPython.display import display\n",
    "        display(results_df.sort_values(by='mean_test_score', ascending=False).head(10))\n",
    "    else:\n",
    "        print(\"\\nTop 10 CV Results (sorted by mean_test_score):\")\n",
    "        print(results_df.sort_values(by='mean_test_score', ascending=False).head(10))\n",
    "    \n",
    "    print(\"\\n=== Winner Summary ===\")\n",
    "    print(f\"The best MLPClassifier is obtained with parameters: {best_params}\")\n",
    "    print(f\"Final Aggregated Score: {final_aggregated_score:.4f} (\"\n",
    "          \"combining Test Accuracy, Test F1, Mean CV Accuracy, and Test ROC AUC if available, \"\n",
    "          f\"with an overfitting penalty if Perf Gap > {overfitting_threshold}).\")\n",
    "    \n",
    "    return best_estimator, final_aggregated_score, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66cef2-2abe-4ca1-be74-73c817346273",
   "metadata": {},
   "source": [
    "Okay, this is a long boy, because MLP takes a looot of energy, so maybe don't run this always. For this reason once it is done, I will put it in markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b9c21-7e47-40e8-b607-4f0c8f7c4e35",
   "metadata": {},
   "source": [
    "best_mlp, aggregated_score, best_hyperparams = tune_mlp_classifier(\n",
    "    df=df_2,\n",
    "    target_column='Survived',\n",
    "    test_size=0.3,                # Using 30% of data for testing\n",
    "    random_state=52,              # Ensures reproducibility\n",
    "    param_grid=None,              # Let the function generate the grid based on complexity\n",
    "    scoring='accuracy',           # Primary metric for grid search\n",
    "    cv=5,                         # 5-fold cross-validation\n",
    "    n_jobs=-1,                    # Utilize all available CPU cores\n",
    "    overfitting_threshold=0.1,    # Threshold for penalizing overfitting\n",
    "    complexity=5                  # Moderate complexity level\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc4b3d7-fee9-4715-9710-29a6257a87ef",
   "metadata": {},
   "source": [
    "The best MLPClassifier is obtained with parameters: \n",
    "{'mlp__activation': 'tanh', 'mlp__alpha': 0.1, 'mlp__hidden_layer_sizes': (100,), \n",
    "\n",
    "'mlp__learning_rate_init': 0.1, 'mlp__solver': 'sgd'}\n",
    "\n",
    "Final Aggregated Score: 0.8166 \n",
    "\n",
    "(combining Test Accuracy, Test F1, Mean CV Accuracy, and Test ROC AUC if available, with an overfitting penalty if Perf Gap > 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a360d0-43f9-49b8-af63-5dbaf17fab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score,\n",
    "    precision_score, recall_score, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # To ignore any warnings during model training\n",
    "\n",
    "def test_tuned_mlp(*dfs, target_column, test_size=0.2, random_state=52, \n",
    "                  names=None, overfitting_threshold=0.1, tuned_params=None, cv=5, scoring='accuracy'):\n",
    "    \"\"\"\n",
    "    Tests a tuned MLPClassifier on multiple DataFrames and prints relevant metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        *dfs: One or more pandas DataFrames containing features and the target column.\n",
    "        target_column (str): The name of the target column (binary classification required).\n",
    "        test_size (float): Fraction of data to be used as test set (default=0.2).\n",
    "        random_state (int): Seed for reproducibility (default=52).\n",
    "        names (list or None): Optional list of names for the DataFrames. If None, defaults to df_1, df_2, etc.\n",
    "        overfitting_threshold (float): Threshold to identify overfitting based on performance gap (default=0.1).\n",
    "        tuned_params (dict or None): Dictionary of tuned parameters for MLPClassifier. If None, defaults are used.\n",
    "        cv (int): Number of cross-validation folds (default=5).\n",
    "        scoring (str): Scoring metric for cross-validation (default='accuracy').\n",
    "    \n",
    "    Returns:\n",
    "        results (dict): Dictionary containing metrics for each DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.base import clone\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Handle naming: auto-generate names if none or too few are provided.\n",
    "    if names is None:\n",
    "        names = [f\"df_{i+1}\" for i in range(len(dfs))]\n",
    "    else:\n",
    "        names = list(names)\n",
    "        if len(names) < len(dfs):\n",
    "            for i in range(len(dfs) - len(names)):\n",
    "                names.append(f\"df_{len(names) + i + 1}\")\n",
    "        elif len(names) > len(dfs):\n",
    "            print(\"Warning: More names provided than DataFrames. Extra names will be ignored.\")\n",
    "            names = names[:len(dfs)]\n",
    "    \n",
    "    # Define default tuned parameters if none are provided\n",
    "    if tuned_params is None:\n",
    "        tuned_params = {\n",
    "            'activation': 'relu',\n",
    "            'alpha': 0.0001,\n",
    "            'hidden_layer_sizes': (100,)\n",
    "        }\n",
    "        print(\"No tuned parameters provided. Using default parameters for MLPClassifier.\")\n",
    "    else:\n",
    "        print(f\"Using tuned parameters for MLPClassifier: {tuned_params}\")\n",
    "    \n",
    "    # Iterate over each DataFrame\n",
    "    for idx, df in enumerate(dfs):\n",
    "        current_name = names[idx]\n",
    "        print(f\"\\n=== Evaluating on {current_name} ===\")\n",
    "        \n",
    "        # Check if target column exists\n",
    "        if target_column not in df.columns:\n",
    "            print(f\"Error: Target column '{target_column}' not found in '{current_name}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Extract features and target\n",
    "        X = df.drop(columns=[target_column])\n",
    "        y = df[target_column]\n",
    "        \n",
    "        # Check if target is binary\n",
    "        if y.nunique() != 2:\n",
    "            print(f\"Error: Target column '{target_column}' in '{current_name}' is not binary. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        print(f\"Training set size: {X_train.shape[0]}\")\n",
    "        print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "        \n",
    "        # Define the Pipeline with Scaling and MLPClassifier\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('mlp', MLPClassifier(random_state=random_state, max_iter=500, **tuned_params))\n",
    "        ])\n",
    "        \n",
    "        # Train the model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        print(\"Model training completed.\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_train_pred = pipeline.predict(X_train)\n",
    "        y_test_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        train_f1 = f1_score(y_train, y_train_pred)\n",
    "        test_f1 = f1_score(y_test, y_test_pred)\n",
    "        train_precision = precision_score(y_train, y_train_pred, zero_division=0)\n",
    "        test_precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "        train_recall = recall_score(y_train, y_train_pred, zero_division=0)\n",
    "        test_recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "        \n",
    "        # ROC AUC\n",
    "        try:\n",
    "            if hasattr(pipeline.named_steps['mlp'], \"predict_proba\"):\n",
    "                y_test_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "                y_train_proba = pipeline.predict_proba(X_train)[:, 1]\n",
    "            elif hasattr(pipeline.named_steps['mlp'], \"decision_function\"):\n",
    "                y_test_proba = pipeline.decision_function(X_test)\n",
    "                y_train_proba = pipeline.decision_function(X_train)\n",
    "            else:\n",
    "                y_test_proba = y_test_pred\n",
    "                y_train_proba = y_train_pred\n",
    "            train_roc_auc = roc_auc_score(y_train, y_train_proba)\n",
    "            test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "        except Exception as e:\n",
    "            train_roc_auc = None\n",
    "            test_roc_auc = None\n",
    "            print(f\"ROC AUC could not be computed: {e}\")\n",
    "        \n",
    "        # Cross-Validation on Training Set\n",
    "        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring=scoring, n_jobs=-1)\n",
    "        mean_cv_accuracy = cv_scores.mean()\n",
    "        \n",
    "        # Performance Gap\n",
    "        performance_gap = train_accuracy - test_accuracy\n",
    "        \n",
    "        # Aggregated Score\n",
    "        metrics_to_average = [test_accuracy, test_f1, mean_cv_accuracy]\n",
    "        if test_roc_auc is not None:\n",
    "            metrics_to_average.append(test_roc_auc)\n",
    "        base_score = np.mean(metrics_to_average)\n",
    "        \n",
    "        # Overfitting Penalty\n",
    "        penalty = 0\n",
    "        if performance_gap > overfitting_threshold:\n",
    "            penalty = 0.5 * performance_gap  # You can adjust the multiplier as needed\n",
    "            analysis = f\"Overfitting detected (Performance Gap: {performance_gap:.4f}). Aggregated score penalized.\"\n",
    "        elif (test_accuracy - train_accuracy) > overfitting_threshold:\n",
    "            analysis = f\"Possible underfitting (Performance Gap: {performance_gap:.4f}). Consider model complexity or data quality.\"\n",
    "        else:\n",
    "            analysis = \"Good generalization.\"\n",
    "        \n",
    "        # Final Aggregated Score\n",
    "        aggregated_score = base_score - penalty\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_test_pred)\n",
    "        \n",
    "        # Classification Report\n",
    "        class_report = classification_report(y_test, y_test_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[current_name] = {\n",
    "            \"Training Accuracy\": round(train_accuracy, 4),\n",
    "            \"Test Accuracy\": round(test_accuracy, 4),\n",
    "            \"Training F1 Score\": round(train_f1, 4),\n",
    "            \"Test F1 Score\": round(test_f1, 4),\n",
    "            \"Training Precision\": round(train_precision, 4),\n",
    "            \"Test Precision\": round(test_precision, 4),\n",
    "            \"Training Recall\": round(train_recall, 4),\n",
    "            \"Test Recall\": round(test_recall, 4),\n",
    "            \"Training ROC AUC\": round(train_roc_auc, 4) if train_roc_auc is not None else \"Not Available\",\n",
    "            \"Test ROC AUC\": round(test_roc_auc, 4) if test_roc_auc is not None else \"Not Available\",\n",
    "            \"Cross-Validation Scores\": [round(score, 4) for score in cv_scores],\n",
    "            \"Mean CV Accuracy\": round(mean_cv_accuracy, 4),\n",
    "            \"Performance Gap\": round(performance_gap, 4),\n",
    "            \"Aggregated Score\": round(base_score, 4),\n",
    "            \"Overfitting Penalty\": round(penalty, 4),\n",
    "            \"Final Aggregated Score\": round(aggregated_score, 4),\n",
    "            \"Analysis\": analysis,\n",
    "            \"Confusion Matrix\": cm,\n",
    "            \"Classification Report\": class_report\n",
    "        }\n",
    "        \n",
    "        # Print metrics\n",
    "        print(\"\\n--- Evaluation Metrics ---\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(f\"Training F1 Score: {train_f1:.4f}\")\n",
    "        print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "        print(f\"Training Precision: {train_precision:.4f}\")\n",
    "        print(f\"Test Precision: {test_precision:.4f}\")\n",
    "        print(f\"Training Recall: {train_recall:.4f}\")\n",
    "        print(f\"Test Recall: {test_recall:.4f}\")\n",
    "        print(f\"Training ROC AUC: {train_roc_auc:.4f}\" if train_roc_auc is not None else \"Training ROC AUC: Not Available\")\n",
    "        print(f\"Test ROC AUC: {test_roc_auc:.4f}\" if test_roc_auc is not None else \"Test ROC AUC: Not Available\")\n",
    "        print(f\"Mean CV Accuracy: {mean_cv_accuracy:.4f}\")\n",
    "        print(f\"Performance Gap (Train - Test Accuracy): {performance_gap:.4f}\")\n",
    "        print(f\"Aggregated Score (before penalty): {base_score:.4f}\")\n",
    "        print(f\"Overfitting Penalty: {penalty:.4f}\")\n",
    "        print(f\"Final Aggregated Score: {aggregated_score:.4f}\")\n",
    "        print(f\"Analysis: {analysis}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(class_report)\n",
    "    \n",
    "    # Return the results dictionary\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c14b4-fe2e-43a3-bbb5-b0c5d0525a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = {\n",
    "    'activation': 'tanh',\n",
    "    'alpha': 0.1,\n",
    "    'hidden_layer_sizes': (100,)\n",
    "}\n",
    "\n",
    "\n",
    "results = test_tuned_mlp(\n",
    "    df_2,\n",
    "    target_column='Survived',\n",
    "    test_size=0.2,\n",
    "    random_state=52,\n",
    "    names=['Model1', 'Model2', 'Model3'],\n",
    "    overfitting_threshold=0.1,\n",
    "    tuned_params=tuned_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f512f5-6ff2-441a-865d-0fb2a125231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = {\n",
    "    'activation': 'tanh',\n",
    "    'alpha': 0.1,\n",
    "    'hidden_layer_sizes': (100,)\n",
    "}\n",
    "\n",
    "\n",
    "results = test_tuned_mlp(\n",
    "    df_1,\n",
    "    target_column='Survived',\n",
    "    test_size=0.2,\n",
    "    random_state=52,\n",
    "    names=['Model1', 'Model2', 'Model3'],\n",
    "    overfitting_threshold=0.1,\n",
    "    tuned_params=tuned_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c887f-4f20-4cd8-9835-a5e6e78afe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = {\n",
    "    'activation': 'tanh',\n",
    "    'alpha': 0.1,\n",
    "    'hidden_layer_sizes': (100,)\n",
    "}\n",
    "\n",
    "\n",
    "results = test_tuned_mlp(\n",
    "    df,\n",
    "    target_column='Survived',\n",
    "    test_size=0.2,\n",
    "    random_state=52,\n",
    "    names=['Model1', 'Model2', 'Model3'],\n",
    "    overfitting_threshold=0.1,\n",
    "    tuned_params=tuned_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498de9c-8038-4b89-9b67-80ff121b0366",
   "metadata": {},
   "source": [
    "False positive and false negatives holds less meaning here, than in - for example - a healthcare modell. So, we try to maximize the accuracy and minimize the false predictions. Thats that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e68ec-9c00-4ccb-8c30-c225e494425c",
   "metadata": {},
   "source": [
    "### Ok let's save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22ee68-0bfb-4388-a986-a5cfd91d0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Tuned parameters for MLPClassifier\n",
    "tuned_params = {\n",
    "    'activation': 'tanh',\n",
    "    'alpha': 0.1,\n",
    "    'hidden_layer_sizes': (100,)\n",
    "}\n",
    "\n",
    "# Using df_2 as the dataset\n",
    "df = df_2\n",
    "target_column = 'Survived'\n",
    "test_size = 0.2\n",
    "random_state = 52\n",
    "\n",
    "# Splitting the dataset\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPClassifier(random_state=random_state, max_iter=500, **tuned_params))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred, zero_division=0)\n",
    "test_precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "train_recall = recall_score(y_train, y_train_pred, zero_division=0)\n",
    "test_recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "\n",
    "try:\n",
    "    y_train_proba = pipeline.predict_proba(X_train)[:, 1]\n",
    "    y_test_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    train_roc_auc = roc_auc_score(y_train, y_train_proba)\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "except AttributeError:\n",
    "    train_roc_auc = None\n",
    "    test_roc_auc = None\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Training F1 Score:\", train_f1)\n",
    "print(\"Test F1 Score:\", test_f1)\n",
    "print(\"Training Precision:\", train_precision)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Training Recall:\", train_recall)\n",
    "print(\"Test Recall:\", test_recall)\n",
    "if train_roc_auc and test_roc_auc:\n",
    "    print(\"Training ROC AUC:\", train_roc_auc)\n",
    "    print(\"Test ROC AUC:\", test_roc_auc)\n",
    "\n",
    "# Save the model in .pkl format\n",
    "model_path = 'best_tuned_mlp_model.pkl'\n",
    "with open(model_path, 'wb') as file:\n",
    "    pickle.dump(pipeline, file)\n",
    "print(f\"Model saved successfully as '{model_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d44886-bc07-440c-822c-3d5e6984d8d3",
   "metadata": {},
   "source": [
    "## LETS UPLOAD IT BABY!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5570655-76d4-4723-853d-a0fd9f8744b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf2c36-4797-46a2-858a-cb29e9ae152d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# The go to CMD:\n",
    "\n",
    "pip install pipreqs\n",
    "\n",
    "pipreqs C:\\Users\\Administrator\\Project_Titanic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0dfba8-0d35-48b5-91f8-20f72fb2b6be",
   "metadata": {},
   "source": [
    "Dang baby, this pipreqs work :O "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d7b2d-2f43-42bb-ba16-ab75fe71c851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7c94255-969b-4bd8-b2b2-cac793bd94c8",
   "metadata": {},
   "source": [
    "# Upgrade pip\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Update the required libraries to the latest versions\n",
    "!pip install --upgrade numpy pandas scikit-learn xgboost lightgbm catboost matplotlib seaborn joblib flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ed18b-d5b6-490d-bc61-26db0f7d923d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d99875b-a41a-4a5d-93fb-878e6258dc47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
